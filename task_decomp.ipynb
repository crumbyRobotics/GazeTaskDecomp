{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Optional: Download sample dataset from GDrive (7.07GB)\n",
    "\"\"\"\n",
    "%pip install --upgrade gdown\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import gdown\n",
    "\n",
    "file_id = '1dmOHCXq7CvSoY1mEq0ISvKxJA_kmQQkG'\n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "output_path = 'dataset/file.zip'\n",
    "gdown.download(download_url, output_path, quiet=False)\n",
    "\n",
    "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('dataset')\n",
    "os.remove(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = sorted([os.path.join(\"dataset\", f) for f in os.listdir(\"dataset\") if \".h5\" in f])\n",
    "print(\"num file:\", len(file_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download CLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "device = torch.device(\"cuda:0\")\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CLIP\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (for demonstrations recorded in 10 Hz)\n",
    "window_size = 20\n",
    "transition_thresh = 50\n",
    "transition_thresh2 = 0.04\n",
    "min_seg_size = round(window_size * 1.5)\n",
    "crop_size_original = 256  # px in 1280x720 image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Measure Change Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gaze_info(\n",
    "    demo_file: str,\n",
    "    window_size: int,\n",
    "    crop_size_original: int,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    "    model,\n",
    "    preprocess,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    with h5py.File(demo_file, \"r\") as demo:\n",
    "        eps_steps = len(demo[\"left_img\"])\n",
    "\n",
    "        _, H, W, C = demo[\"left_img\"].shape\n",
    "        crop_size = int(crop_size_original * W / 1280)\n",
    "\n",
    "        gazes = np.round(demo[\"gaze\"]).astype(np.int64).reshape(-1, 2, 2)  # (N, 2, 2)\n",
    "        gazes = np.clip(gazes, [0, 0], [W, H])\n",
    "\n",
    "        gazes_median = np.array([np.median(gazes[max(0, i - window_size) : i + window_size + 1], axis=0) for i in range(len(gazes))]).astype(np.int64)  # (N, 2, 2)\n",
    "\n",
    "        imgs = []\n",
    "        for step in range(eps_steps):\n",
    "            img = np.stack([demo[\"left_img\"][step], demo[\"right_img\"][step]])  # (2, H, W, C)\n",
    "            img = np.ascontiguousarray(img[:, :, :, [2, 1, 0]])  # BGR2RGB\n",
    "\n",
    "            pad_img = np.zeros((2, H + crop_size, W + crop_size, C), dtype=np.uint8)\n",
    "            pad_img[:, crop_size // 2 : H + crop_size // 2, crop_size // 2 : W + crop_size // 2] = img\n",
    "\n",
    "            gaze = gazes_median[step]\n",
    "\n",
    "            gaze_img = []\n",
    "            for lr in range(2):\n",
    "                gaze_img.append(pad_img[lr, gaze[lr, 1] : gaze[lr, 1] + crop_size, gaze[lr, 0] : gaze[lr, 0] + crop_size])  # (crop_size, crop_size, C)\n",
    "            gaze_img = np.stack(gaze_img)  # (2, crop_size, crop_size, C)\n",
    "            # display(Image.fromarray(gaze_img[0]))\n",
    "\n",
    "            imgs.append(gaze_img)\n",
    "        imgs = np.stack(imgs)  # (N, 2, crop_size, crop_size, C)\n",
    "\n",
    "    image_features = []\n",
    "    for i in range((eps_steps + (batch_size // 2 - 1)) // (batch_size // 2)):\n",
    "        image = imgs[i * batch_size // 2 : (i + 1) * batch_size // 2].reshape(-1, *imgs.shape[2:])  # (B * 2, H, W, C)\n",
    "        image = torch.stack([preprocess(Image.fromarray(im)) for im in image])  # (B * 2, H, W, C)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image = image.to(device)\n",
    "            image_feature = model.encode_image(image, normalize=True)  # (B * 2, feature_dim)\n",
    "        image_features.append(image_feature.detach().cpu().numpy().reshape(-1, 2, image_feature.shape[1]))  # (B, 2, feature_dim)\n",
    "    image_features = np.concatenate(image_features)  # (N, 2, feature_dim)\n",
    "\n",
    "    return gazes_median, image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_score(gazes_median: np.ndarray, image_features: np.ndarray) -> tuple[list, list]:\n",
    "    score_gazes = []\n",
    "    score_features = []\n",
    "    for step in range(len(gazes_median) - 1):\n",
    "        # Euclid distance of gaze\n",
    "        gaze_diff = (gazes_median[step + 1] - gazes_median[step]).reshape(-1)  # (4,)\n",
    "\n",
    "        # Cosine similarity of image_features\n",
    "        image_feature_before = image_features[step]  # (2, feature_dim)\n",
    "        image_feature_after = image_features[step + 1]  # (2, feature_dim)\n",
    "        feature_similarity = -np.log((image_feature_before @ image_feature_after.T).diagonal() + 1 + 1e-6) + np.log(2)  # (2,)\n",
    "\n",
    "        # Calculate scores\n",
    "        score_gaze = np.linalg.norm(gaze_diff)\n",
    "        score_feature = feature_similarity.mean(0)\n",
    "\n",
    "        score_gazes.append(score_gaze)\n",
    "        score_features.append(score_feature)\n",
    "\n",
    "    score_gazes = [0] + score_gazes\n",
    "    score_features = [0] + score_features\n",
    "\n",
    "    return score_gazes, score_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure change scores of gaze information\n",
    "data_score_gazes = []\n",
    "data_score_features = []\n",
    "data_eps_steps = []\n",
    "for eps_idx, file_name in enumerate(file_names):\n",
    "    # Extract gaze information from demonstration file\n",
    "    gazes_median, image_features = extract_gaze_info(file_name, window_size, crop_size_original, batch_size, device, model, preprocess)\n",
    "    print(f\"Demo info: {eps_idx} [file_name={file_name}, eps_step={len(gazes_median)}]\")\n",
    "\n",
    "    # Calculate change scores\n",
    "    score_gazes, score_features = change_score(gazes_median, image_features)\n",
    "\n",
    "    # Data for task decomposition\n",
    "    data_score_gazes.append(score_gazes)\n",
    "    data_score_features.append(score_features)\n",
    "    data_eps_steps.append(len(gazes_median))\n",
    "data_eps_steps = np.cumsum([0] + data_eps_steps)  # (N_eps + 1,)\n",
    "print(data_eps_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Detect Gaze Transitions by Change Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_transition(\n",
    "    score_gazes: list,\n",
    "    score_features: list,\n",
    "    thresh: float,\n",
    "    thresh2: float,\n",
    "    min_seg_size: int,\n",
    ") -> list:\n",
    "    eps_steps = len(score_gazes)\n",
    "\n",
    "    # Detecting gaze transition\n",
    "    seg_steps = []\n",
    "    for step in range(eps_steps - 1):\n",
    "        score_gaze = score_gazes[step]\n",
    "        score_feature = score_features[step]\n",
    "        if score_gaze > thresh and score_feature > thresh2:\n",
    "            seg_steps.append(step)\n",
    "\n",
    "    seg_steps = [0] + seg_steps + [eps_steps]\n",
    "\n",
    "    # Thinning seg points (Delete segments that are too small)\n",
    "    total_scores = thresh * np.array(score_gazes) + thresh2 * np.array(score_features)\n",
    "    seg_sizes = np.diff(seg_steps)\n",
    "    while not np.all(seg_sizes >= min_seg_size):\n",
    "        if len(seg_steps) < 4:\n",
    "            break\n",
    "        \n",
    "        assert len(seg_sizes) > 1\n",
    "        min_i = np.argmin(seg_sizes)\n",
    "        if min_i == 0:\n",
    "            seg_steps.pop(min_i + 1)\n",
    "        elif min_i == len(seg_sizes) - 1:\n",
    "            seg_steps.pop(min_i)\n",
    "        else:\n",
    "            if total_scores[seg_steps[min_i + 1]] < total_scores[seg_steps[min_i]]:\n",
    "                seg_steps.pop(min_i + 1)\n",
    "            else:\n",
    "                seg_steps.pop(min_i)\n",
    "        seg_sizes = np.diff(seg_steps)\n",
    "\n",
    "    return seg_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection(\n",
    "    file_name: str,\n",
    "    score_gazes: list,\n",
    "    score_features: list,\n",
    "    thresh: float,\n",
    "    thresh2: float,\n",
    "    seg_steps: list,\n",
    "    crop_size_original: int,\n",
    "):\n",
    "    plt.plot(score_gazes, label=\"s_pos\", c=\"sandybrown\", lw=2)\n",
    "    plt.plot(np.array(score_features) * thresh / thresh2, label=\"s_feat\", c=\"cornflowerblue\", lw=1)\n",
    "    plt.hlines([thresh], 0, len(score_gazes), colors=\"black\", lw=2)\n",
    "    plt.vlines(np.array(seg_steps[1:-1]), 0, 30, colors=\"red\", alpha=0.7, lw=3)\n",
    "    plt.ylim(0, 150.0)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    with h5py.File(file_name, \"r\") as demo:\n",
    "        for step in seg_steps[1:-1]:\n",
    "            NN = 9\n",
    "            imgs = np.stack([demo[\"left_img\"][step - NN : step + NN + 1], demo[\"right_img\"][step - NN : step + NN + 1]], axis=1)  # (N, 2, H, W, C)\n",
    "            imgs = np.ascontiguousarray(imgs[..., [2, 1, 0]])  # BGR2RGB\n",
    "            gazes = np.round(demo[\"gaze\"][step - NN : step + NN + 1]).astype(np.int64).reshape(-1, 2, 2)  # (N, 2, 2)\n",
    "\n",
    "            N, _, H, W, C = imgs.shape\n",
    "            crop_size = int(crop_size_original * W / 1280)\n",
    "\n",
    "            pad_imgs = np.zeros((N, 2, H + crop_size, W + crop_size, C), dtype=np.uint8)\n",
    "            pad_imgs[:, :, crop_size // 2 : H + crop_size // 2, crop_size // 2 : W + crop_size // 2] = imgs\n",
    "\n",
    "            gazes = np.clip(gazes, [0, 0], [W, H])\n",
    "\n",
    "            gaze_imgs = []\n",
    "            for n in range(N):\n",
    "                for lr in range(2):\n",
    "                    gaze_imgs.append(pad_imgs[n, lr, gazes[n, lr, 1] : gazes[n, lr, 1] + crop_size, gazes[n, lr, 0] : gazes[n, lr, 0] + crop_size])  # (crop_size, crop_size, C)\n",
    "            gaze_imgs = np.stack(gaze_imgs).reshape(N, 2, crop_size, crop_size, C)  # (N, 2, crop_size, crop_size, C)\n",
    "\n",
    "            print(f\"step: {step}, scores: {thresh * np.array(score_gazes[step]) + thresh2 * np.array(score_features[step])} ({score_gazes[step]} + {score_features[step]})\")\n",
    "            display(Image.fromarray(np.concatenate(gaze_imgs[:, 0], axis=1)).resize((64 * N, 64)))\n",
    "            # display(Image.fromarray(np.concatenate(gaze_imgs[:, 1], axis=1)).resize((64 * N, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect gaze transition using change scores\n",
    "data_seg_steps = []\n",
    "for eps_idx, file_name in enumerate(file_names):\n",
    "    if eps_idx == len(data_score_gazes):\n",
    "        break\n",
    "\n",
    "    print(\"===================================================================\")\n",
    "    print(f\"Demo info: {eps_idx} [file_name={file_name}, eps_step={len(data_score_gazes[eps_idx])}]\")\n",
    "\n",
    "    seg_steps = detect_transition(\n",
    "        data_score_gazes[eps_idx],\n",
    "        data_score_features[eps_idx],\n",
    "        transition_thresh,\n",
    "        transition_thresh2,\n",
    "        min_seg_size,\n",
    "    )\n",
    "    data_seg_steps.append(seg_steps)\n",
    "\n",
    "    print(\"seg_steps:\", seg_steps)\n",
    "\n",
    "    visualize_detection(\n",
    "        file_name,\n",
    "        data_score_gazes[eps_idx],\n",
    "        data_score_features[eps_idx],\n",
    "        transition_thresh,\n",
    "        transition_thresh2,\n",
    "        seg_steps,\n",
    "        crop_size_original,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Mode of Segment Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segs, num_seg_count = np.unique([len(seg_steps) for seg_steps in data_seg_steps[: eps_idx + 1]], return_counts=True)\n",
    "seg_num = num_segs[np.argmax(num_seg_count)] - 1  # remove 0 and eps_steps\n",
    "seg_nums = np.array([seg_num for _ in file_names])\n",
    "\n",
    "print(f\"Majority of sub-task counts: {seg_num} (num_segs: {num_segs - 1}, count: {num_seg_count})\")\n",
    "print(\"seg_nums:\\n\", seg_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Refine Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_count = 0\n",
    "delta = 0.01\n",
    "max_iter = 500\n",
    "for eps_idx, file_name in enumerate(file_names):\n",
    "    if eps_idx == len(data_score_gazes):\n",
    "        break\n",
    "\n",
    "    new_thresh = transition_thresh\n",
    "    new_thresh2 = transition_thresh2\n",
    "\n",
    "    seg_steps = data_seg_steps[eps_idx].copy()\n",
    "\n",
    "    # Shortage than seg_num: decrease thresh little by little until seg_num is reached (both thresh are varied by a uniform small percentage).\n",
    "    if len(seg_steps) - 1 < seg_nums[eps_idx]:\n",
    "        for _ in range(max_iter):\n",
    "            new_thresh = new_thresh - delta * transition_thresh\n",
    "            new_thresh2 = new_thresh2 - delta * transition_thresh2\n",
    "\n",
    "            seg_steps = detect_transition(\n",
    "                data_score_gazes[eps_idx],\n",
    "                data_score_features[eps_idx],\n",
    "                new_thresh,\n",
    "                new_thresh2,\n",
    "                min_seg_size,\n",
    "            )\n",
    "\n",
    "            if len(seg_steps) - 1 == seg_nums[eps_idx]:\n",
    "                break\n",
    "\n",
    "    # More than seg_num: increase thresh little by little until seg_num is reached\n",
    "    elif len(seg_steps) - 1 > seg_nums[eps_idx]:\n",
    "        for _ in range(max_iter):\n",
    "            new_thresh = new_thresh + delta * transition_thresh\n",
    "            new_thresh2 = new_thresh2 + delta * transition_thresh2\n",
    "\n",
    "            seg_steps = detect_transition(\n",
    "                data_score_gazes[eps_idx],\n",
    "                data_score_features[eps_idx],\n",
    "                new_thresh,\n",
    "                new_thresh2,\n",
    "                min_seg_size,\n",
    "            )\n",
    "\n",
    "            if len(seg_steps) - 1 == seg_nums[eps_idx]:\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    print(\"===================================================================\")\n",
    "    print(f\"Demo info: {eps_idx} [file_name={file_name}, eps_step={len(data_score_gazes[eps_idx])}]\")\n",
    "    if len(seg_steps) - 1 == seg_nums[eps_idx]:\n",
    "        print(f\"Refinement result: {data_seg_steps[eps_idx]} --> {seg_steps}\")\n",
    "    else:\n",
    "        print(f\"[Failed to refine segmentation: {seg_steps} (init={data_seg_steps[eps_idx]})]\")\n",
    "        failure_count += 1\n",
    "\n",
    "    data_seg_steps[eps_idx] = seg_steps\n",
    "\n",
    "    visualize_detection(\n",
    "        file_name,\n",
    "        data_score_gazes[eps_idx],\n",
    "        data_score_features[eps_idx],\n",
    "        transition_thresh,\n",
    "        transition_thresh2,\n",
    "        seg_steps,\n",
    "        crop_size_original,\n",
    "    )\n",
    "\n",
    "print(\"num failure:\", failure_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save Segmentation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "invalid_episodes = []\n",
    "for eps_idx, file_name in enumerate(file_names):\n",
    "    with h5py.File(file_name, \"r+\") as demo:\n",
    "        if \"change_steps\" in demo:\n",
    "            del demo[\"change_steps\"]\n",
    "\n",
    "        if os.path.basename(file_name) not in invalid_episodes:\n",
    "            if len(data_seg_steps[eps_idx]) - 1 != seg_nums[eps_idx]:\n",
    "                invalid_episodes.append(file_name)\n",
    "                continue\n",
    "            print(f\"episode {eps_idx}: {data_seg_steps[eps_idx]}\")\n",
    "            demo.create_dataset(\"change_steps\", data=np.array(data_seg_steps[eps_idx]))\n",
    "\n",
    "print(\"invalid episodes:\", invalid_episodes)\n",
    "print(\"successful episodes:\", len(file_names) - len(invalid_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Check Saved Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps_idx, file_name in enumerate(file_names):\n",
    "    with h5py.File(file_name, \"r\") as demo:\n",
    "        eps_steps = len(demo[\"left_img\"])\n",
    "\n",
    "        print(\"===================================================================\")\n",
    "        print(f\"Demo info: {eps_idx} [file_name={file_name}, eps_step={eps_steps}]\")\n",
    "\n",
    "        if \"change_steps\" in demo:\n",
    "            change_steps = demo[\"change_steps\"]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Visualize each sub-task\n",
    "        init_step = change_steps[0]\n",
    "        for subtask_idx, change_step in enumerate(change_steps[1:]):\n",
    "            print(f\"[Sub-task {subtask_idx + 1}]\")\n",
    "\n",
    "            NN = 4  # FIXED\n",
    "            interval = (change_step - init_step) // NN\n",
    "            imgs = np.stack([demo[\"left_img\"][[init_step, init_step + interval, init_step + 2 * interval, change_step - 1]], demo[\"right_img\"][[init_step, init_step + interval, init_step + 2 * interval, change_step - 1]]], axis=1)  # (N, 2, H, W, C)\n",
    "            imgs = np.ascontiguousarray(imgs[..., [2, 1, 0]])\n",
    "            gazes = np.round(demo[\"gaze\"][[init_step, init_step + interval, init_step + 2 * interval, change_step - 1]]).astype(np.int64).reshape(-1, 2, 2)  # (N, 2, 2)\n",
    "\n",
    "            N, _, H, W, C = imgs.shape\n",
    "            crop_size = int(crop_size_original * W / 1280)\n",
    "\n",
    "            pad_imgs = np.zeros((N, 2, H + crop_size, W + crop_size, C), dtype=np.uint8)\n",
    "            pad_imgs[:, :, crop_size // 2 : H + crop_size // 2, crop_size // 2 : W + crop_size // 2] = imgs\n",
    "\n",
    "            gazes = np.clip(gazes, [0, 0], [W, H])\n",
    "\n",
    "            masked_imgs = imgs * 0.5\n",
    "            for n in range(N):\n",
    "                for lr in range(2):\n",
    "                    masked_imgs[n, lr, gazes[n, lr, 1] - crop_size // 2 : gazes[n, lr, 1] + crop_size // 2 + 1, gazes[n, lr, 0] - crop_size // 2 : gazes[n, lr, 0] + crop_size // 2 + 1] = imgs[\n",
    "                        n, lr, gazes[n, lr, 1] - crop_size // 2 : gazes[n, lr, 1] + crop_size // 2 + 1, gazes[n, lr, 0] - crop_size // 2 : gazes[n, lr, 0] + crop_size // 2 + 1\n",
    "                    ]\n",
    "            masked_imgs = masked_imgs.astype(np.uint8)\n",
    "\n",
    "            display(Image.fromarray(np.concatenate(masked_imgs[:, 0], axis=1)).resize((320 * N, 180)))\n",
    "\n",
    "            init_step = change_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskdecomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
